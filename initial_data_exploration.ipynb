{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Tensorflow",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "initial_data_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d165de0b0594424a67e90b7be8079b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a4b419c8abbb49b8b7d50164ff146b74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e1c5f806919e415ab42d44f51856e2a3",
              "IPY_MODEL_d80e8a24779f44b7b36b71242cd0e49c"
            ]
          }
        },
        "a4b419c8abbb49b8b7d50164ff146b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1c5f806919e415ab42d44f51856e2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6a3ab4b187e4f72b399f296dd187fa4",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 308,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd40c08751004f898692b142487fba3e"
          }
        },
        "d80e8a24779f44b7b36b71242cd0e49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_488c501f244043eaae66ae855901833c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/308 [01:21&lt;6:58:14, 81.74s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_798b93790fa34f949f80a36585a278c4"
          }
        },
        "e6a3ab4b187e4f72b399f296dd187fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd40c08751004f898692b142487fba3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "488c501f244043eaae66ae855901833c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "798b93790fa34f949f80a36585a278c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3954eb7a15714c65a421e35ead967ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_faedb5f98a57418584e7973cacf768a8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f675c1056d7c4e77a13f79f85348e8ac",
              "IPY_MODEL_bfd2441c1826404aa4c52de8f3170ef5"
            ]
          }
        },
        "faedb5f98a57418584e7973cacf768a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f675c1056d7c4e77a13f79f85348e8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b6c52763c98d496ba0446428e4cebb57",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 860,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 860,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93031049d808418cbab166e56978b178"
          }
        },
        "bfd2441c1826404aa4c52de8f3170ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_374df9eef5124ef59edf38dfa3684c95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 860/860 [03:59&lt;00:00,  3.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d96969a4b13448c5ba10dfff1c1eceff"
          }
        },
        "b6c52763c98d496ba0446428e4cebb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93031049d808418cbab166e56978b178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "374df9eef5124ef59edf38dfa3684c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d96969a4b13448c5ba10dfff1c1eceff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/photographe/cortx_google_nq/blob/master/initial_data_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAg9KwETXdsJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "07a98a32-b555-4009-a53c-a89c7fb9a851"
      },
      "source": [
        "'''\n",
        "prediction_format = \n",
        "\n",
        "  {'predictions': \n",
        "      [\n",
        "        {\n",
        "          'example_id': -2226525965842375672,\n",
        "          'long_answer': {\n",
        "            'start_byte': 62657, 'end_byte': 64776,\n",
        "            'start_token': 391, 'end_token': 604\n",
        "          },\n",
        "          'long_answer_score': 13.5,\n",
        "        },\n",
        "        {\n",
        "        .....\n",
        "        }\n",
        "      ]\n",
        "  }\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprediction_format = \\n\\n  {'predictions': \\n      [\\n        {\\n          'example_id': -2226525965842375672,\\n          'long_answer': {\\n            'start_byte': 62657, 'end_byte': 64776,\\n            'start_token': 391, 'end_token': 604\\n          },\\n          'long_answer_score': 13.5,\\n        },\\n        {\\n        .....\\n        }\\n      ]\\n  }\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVMiSJULXdsP",
        "colab_type": "code",
        "outputId": "b291058a-0dc2-43a6-a119-b2a471036b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZxP2NL0XdsS",
        "colab_type": "code",
        "outputId": "6f7f0e07-4545-4465-d853-8ed4e6ee0988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "import functools\n",
        "import gc\n",
        "import itertools\n",
        "import json\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "import time\n",
        "from typing import Callable, Dict, List, Generator, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.io.json._json import JsonReader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "\n",
        "#from apex import amp\n",
        "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup, BertModel, BertPreTrainedModel\n",
        "#from transformers import DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup, DistilBertModel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGRCOUlqXdsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'processed_sample.json'\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "seed = 42\n",
        "valid_size = 0\n",
        "train_size = 307373 - valid_size\n",
        "\n",
        "chunksize = 1000\n",
        "max_seq_len = 512\n",
        "max_question_len = 64\n",
        "doc_stride = 128\n",
        "\n",
        "num_labels = 2\n",
        "n_epochs = 1\n",
        "lr = 2e-5\n",
        "warmup = 0.05\n",
        "batch_size = 16\n",
        "accumulation_steps = 4\n",
        "\n",
        "bert_model = 'bert-base-uncased'\n",
        "do_lower_case = 'uncased' in bert_model\n",
        "device = torch.device('cuda')\n",
        "\n",
        "output_model_file = 'bert_pytorch.bin'\n",
        "output_optimizer_file = 'bert_pytorch_optimizer.bin'\n",
        "output_amp_file = 'bert_pytorch_amp.bin'\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubVdwpsPXdsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@dataclass\n",
        "class Example(object):\n",
        "    example_id: int\n",
        "    candidates: List[Dict]\n",
        "    annotations: Dict\n",
        "    doc_start: int\n",
        "    question_len: int\n",
        "    tokenized_to_original_index: List[int]\n",
        "    input_ids: List[int]\n",
        "    start_position: int\n",
        "    end_position: int\n",
        "    class_label: str\n",
        "\n",
        "        \n",
        "def convert_data(\n",
        "    line: str,\n",
        "    tokenizer: BertTokenizer,\n",
        "    max_seq_len: int,\n",
        "    max_question_len: int,\n",
        "    doc_stride: int\n",
        ") -> List[Example]:\n",
        "    \"\"\"Convert dictionary data into list of training data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    line : str\n",
        "        Training data.\n",
        "    tokenizer : transformers.BertTokenizer\n",
        "        Tokenizer for encoding texts into ids.\n",
        "    max_seq_len : int\n",
        "        Maximum input sequence length.\n",
        "    max_question_len : int\n",
        "        Maximum input question length.\n",
        "    doc_stride : int\n",
        "        When splitting up a long document into chunks, how much stride to take between chunks.\n",
        "    \"\"\"\n",
        "\n",
        "    def _find_short_range(short_answers: List[Dict]) -> Tuple[int, int]:\n",
        "        answers = pd.DataFrame(short_answers)\n",
        "        start_min = answers['start_token'].min()\n",
        "        end_max = answers['end_token'].max()\n",
        "        return start_min, end_max\n",
        "\n",
        "    # model input\n",
        "    data = json.loads(line)\n",
        "    doc_words = data['document_text'].split()\n",
        "    question_tokens = tokenizer.tokenize(data['question_text'])[:max_question_len]\n",
        "\n",
        "    # tokenized index of i-th original token corresponds to original_to_tokenized_index[i]\n",
        "    # if a token in original text is removed, its tokenized index indicates next token\n",
        "    original_to_tokenized_index = []\n",
        "    tokenized_to_original_index = []\n",
        "    all_doc_tokens = []  # tokenized document text\n",
        "    for i, word in enumerate(doc_words):\n",
        "        original_to_tokenized_index.append(len(all_doc_tokens))\n",
        "        if re.match(r'<.+>', word):  # remove paragraph tag\n",
        "            continue\n",
        "        sub_tokens = tokenizer.tokenize(word)\n",
        "        for sub_token in sub_tokens:\n",
        "            tokenized_to_original_index.append(i)\n",
        "            all_doc_tokens.append(sub_token)\n",
        "\n",
        "    # model output: (class_label, start_position, end_position)\n",
        "    annotations = data['annotations'][0]\n",
        "    if annotations['long_answer']['candidate_index'] != -1:\n",
        "        class_label = 'long'\n",
        "        start_position = annotations['long_answer']['start_token']\n",
        "        end_position = annotations['long_answer']['end_token']\n",
        "    else:\n",
        "        class_label = 'unknown'\n",
        "        start_position = -1\n",
        "        end_position = -1\n",
        "\n",
        "    # convert into tokenized index\n",
        "    if start_position != -1 and end_position != -1:\n",
        "        start_position = original_to_tokenized_index[start_position]\n",
        "        end_position = original_to_tokenized_index[end_position]\n",
        "\n",
        "    # make sure at least one object in `examples`\n",
        "    examples = []\n",
        "    max_doc_len = max_seq_len - len(question_tokens) - 3  # [CLS], [SEP], [SEP]\n",
        "\n",
        "    # take chunks with a stride of `doc_stride`\n",
        "    for doc_start in range(0, len(all_doc_tokens), doc_stride):\n",
        "        doc_end = doc_start + max_doc_len\n",
        "        # if truncated document does not contain annotated range\n",
        "        if not (doc_start <= start_position and end_position <= doc_end):\n",
        "            start, end, label = -1, -1, 'unknown'\n",
        "        else:\n",
        "            start = start_position - doc_start + len(question_tokens) + 2\n",
        "            end = end_position - doc_start + len(question_tokens) + 2\n",
        "            label = class_label\n",
        "\n",
        "        assert -1 <= start < max_seq_len, f'start position is out of range: {start}'\n",
        "        assert -1 <= end < max_seq_len, f'end position is out of range: {end}'\n",
        "\n",
        "        doc_tokens = all_doc_tokens[doc_start:doc_end]\n",
        "        input_tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + doc_tokens + ['[SEP]']\n",
        "        examples.append(\n",
        "            Example(\n",
        "                example_id=data['example_id'],\n",
        "                candidates=data['long_answer_candidates'],\n",
        "                annotations=annotations,\n",
        "                doc_start=doc_start,\n",
        "                question_len=len(question_tokens),\n",
        "                tokenized_to_original_index=tokenized_to_original_index,\n",
        "                input_ids=tokenizer.convert_tokens_to_ids(input_tokens),\n",
        "                start_position=start,\n",
        "                end_position=end,\n",
        "                class_label=label\n",
        "        ))\n",
        "\n",
        "    return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ENvPQxMXdsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class JsonChunkReader(JsonReader):    \n",
        "    def __init__(\n",
        "        self,\n",
        "        filepath_or_buffer: str,\n",
        "        convert_data: Callable[[str], List[Example]],\n",
        "        orient: str = None,\n",
        "        typ: str = 'frame',\n",
        "        dtype: bool = None,\n",
        "        convert_axes: bool = None,\n",
        "        convert_dates: bool = True,\n",
        "        keep_default_dates: bool = True,\n",
        "        numpy: bool = False,\n",
        "        precise_float: bool = False,\n",
        "        date_unit: str = None,\n",
        "        encoding: str = None,\n",
        "        lines: bool = True,\n",
        "        chunksize: int = 2000,\n",
        "        compression: str = None,\n",
        "    ):\n",
        "        super(JsonChunkReader, self).__init__(\n",
        "            str(filepath_or_buffer),\n",
        "            orient=orient, typ=typ, dtype=dtype,\n",
        "            convert_axes=convert_axes,\n",
        "            convert_dates=convert_dates,\n",
        "            keep_default_dates=keep_default_dates,\n",
        "            numpy=numpy, precise_float=precise_float,\n",
        "            date_unit=date_unit, encoding=encoding,\n",
        "            lines=lines, chunksize=chunksize,\n",
        "            compression=compression\n",
        "        )\n",
        "        self.convert_data = convert_data\n",
        "        \n",
        "    def __next__(self):\n",
        "        lines = list(itertools.islice(self.data, self.chunksize))\n",
        "        if lines:\n",
        "            with Pool(2) as p:\n",
        "                obj = p.map(self.convert_data, lines)\n",
        "            return obj\n",
        "\n",
        "        self.close()\n",
        "        raise StopIteration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwMtLMh-Xdsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, examples: List[Example]):\n",
        "        self.examples = examples\n",
        "        \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        annotated = list(\n",
        "            filter(lambda example: example.class_label != 'unknown', self.examples[index]))\n",
        "        if len(annotated) == 0:\n",
        "            return random.choice(self.examples[index])\n",
        "        return random.choice(annotated)\n",
        "\n",
        "    \n",
        "def collate_fn(examples: List[Example]) -> List[List[torch.Tensor]]:\n",
        "    # input tokens\n",
        "    max_len = max([len(example.input_ids) for example in examples])\n",
        "    tokens = np.zeros((len(examples), max_len), dtype=np.int64)\n",
        "    token_type_ids = np.ones((len(examples), max_len), dtype=np.int64)\n",
        "    for i, example in enumerate(examples):\n",
        "        row = example.input_ids\n",
        "        tokens[i, :len(row)] = row\n",
        "        token_type_id = [0 if i <= row.index(102) else 1\n",
        "                         for i in range(len(row))]  # 102 corresponds to [SEP]\n",
        "        token_type_ids[i, :len(row)] = token_type_id\n",
        "    attention_mask = tokens > 0\n",
        "    inputs = [torch.from_numpy(tokens),\n",
        "              torch.from_numpy(attention_mask),\n",
        "              torch.from_numpy(token_type_ids)]\n",
        "\n",
        "    # output labels\n",
        "    all_labels = ['long', 'unknown']\n",
        "    start_positions = np.array([example.start_position for example in examples])\n",
        "    end_positions = np.array([example.end_position for example in examples])\n",
        "    class_labels = [all_labels.index(example.class_label) for example in examples]\n",
        "    start_positions = np.where(start_positions >= max_len, -1, start_positions)\n",
        "    end_positions = np.where(end_positions >= max_len, -1, end_positions)\n",
        "    labels = [torch.LongTensor(start_positions),\n",
        "              torch.LongTensor(end_positions),\n",
        "              torch.LongTensor(class_labels)]\n",
        "\n",
        "    return [inputs, labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbzvhVZnmzi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for QA and classification tasks.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    config : transformers.BertConfig. Configuration class for BERT.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    start_logits : torch.Tensor with shape (batch_size, sequence_size).\n",
        "        Starting scores of each tokens.\n",
        "    end_logits : torch.Tensor with shape (batch_size, sequence_size).\n",
        "        Ending scores of each tokens.\n",
        "    classifier_logits : torch.Tensor with shape (batch_size, num_classes).\n",
        "        Classification scores of each labels.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)  # start/end\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,)\n",
        "                            #position_ids=position_ids, \n",
        "                            #head_mask=head_mask)\n",
        "        #print(outputs)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        # predict start & end position\n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "    \n",
        "        # classification\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        classifier_logits = self.classifier(pooled_output)\n",
        "\n",
        "        return start_logits, end_logits, classifier_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HYQLxSm3az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds, class_preds = preds\n",
        "    start_labels, end_labels, class_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    class_loss = nn.CrossEntropyLoss()(class_preds, class_labels)\n",
        "    return start_loss + end_loss + class_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83VXsteHm5Ci",
        "colab_type": "code",
        "outputId": "ac4f7380-a12b-4bd3-e428-1ca65878ef2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7d165de0b0594424a67e90b7be8079b4",
            "a4b419c8abbb49b8b7d50164ff146b74",
            "e1c5f806919e415ab42d44f51856e2a3",
            "d80e8a24779f44b7b36b71242cd0e49c",
            "e6a3ab4b187e4f72b399f296dd187fa4",
            "bd40c08751004f898692b142487fba3e",
            "488c501f244043eaae66ae855901833c",
            "798b93790fa34f949f80a36585a278c4"
          ]
        }
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(bert_model, num_labels=2)\n",
        "model = model.to(device)\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "num_train_optimization_steps = int(n_epochs * train_size / batch_size / accumulation_steps)\n",
        "num_warmup_steps = int(num_train_optimization_steps * warmup)\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_optimization_steps)\n",
        "\n",
        "#model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
        "model.zero_grad()\n",
        "model = model.train()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
        "convert_func = functools.partial(convert_data,\n",
        "                                 tokenizer=tokenizer,\n",
        "                                 max_seq_len=max_seq_len,\n",
        "                                 max_question_len=max_question_len,\n",
        "                                 doc_stride=doc_stride)\n",
        "data_reader = JsonChunkReader(DATA_PATH, convert_func, chunksize=chunksize)\n",
        "\n",
        "global_step = 0\n",
        "for examples in tqdm(data_reader, total=int(np.ceil(train_size / chunksize))):\n",
        "    train_dataset = TextDataset(examples)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, attention_mask, token_type_ids = x_batch\n",
        "        # x_batch, attention_mask = x_batch\n",
        "        y_batch = (y.to(device) for y in y_batch)\n",
        "\n",
        "        y_pred = model(x_batch.to(device),\n",
        "                       attention_mask=attention_mask.to(device),\n",
        "                       token_type_ids=token_type_ids.to(device)\n",
        "                       )\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        #    scaled_loss.backward()\n",
        "        if (global_step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "del examples, train_dataset, train_loader\n",
        "gc.collect()\n",
        "\n",
        "torch.save(model.state_dict(), output_model_file)\n",
        "torch.save(optimizer.state_dict(), output_optimizer_file)\n",
        "model.save_pretrained('/content/')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d165de0b0594424a67e90b7be8079b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=308), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj5E65K5xevM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11957b32-02fc-4e35-f79b-765555c3d054"
      },
      "source": [
        "print(f'trained {global_step * batch_size} samples')\n",
        "print(f'training time: {(time.time() - start_time) / 3600:.1f} hours')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trained 208 samples\n",
            "training time: 0.1 hours\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0EF7vf53c6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('/content/', num_labels=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYi3qrMs0Gm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_collate_fn(examples: List[Example]) -> Tuple[List[torch.Tensor], List[Example]]:\n",
        "    # input tokens\n",
        "    max_len = max([len(example.input_ids) for example in examples])\n",
        "    tokens = np.zeros((len(examples), max_len), dtype=np.int64)\n",
        "    token_type_ids = np.ones((len(examples), max_len), dtype=np.int64)\n",
        "    for i, example in enumerate(examples):\n",
        "        row = example.input_ids\n",
        "        tokens[i, :len(row)] = row\n",
        "        token_type_id = [0 if i <= row.index(102) else 1\n",
        "                         for i in range(len(row))]  # 102 corresponds to [SEP]\n",
        "        token_type_ids[i, :len(row)] = token_type_id\n",
        "    attention_mask = tokens > 0\n",
        "    inputs = [torch.from_numpy(tokens),\n",
        "              torch.from_numpy(attention_mask),\n",
        "              torch.from_numpy(token_type_ids)]\n",
        "\n",
        "    return inputs, examples\n",
        "\n",
        "\n",
        "def eval_model(\n",
        "    model: nn.Module,\n",
        "    valid_loader: DataLoader,\n",
        "    device: torch.device = torch.device('cuda')\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Compute validation score.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model for prediction.\n",
        "    valid_loader : DataLoader\n",
        "        Data loader of validation data.\n",
        "    device : torch.device, optional\n",
        "        Device for computation.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Scores of validation data.\n",
        "        `long_score`: score of long answers\n",
        "        `overall_score`: score of the competition metric\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        result = Result()\n",
        "        for inputs, examples in tqdm(valid_loader):\n",
        "            input_ids, attention_mask, token_type_ids = inputs\n",
        "            y_preds = model(input_ids.to(device),\n",
        "                            attention_mask.to(device),\n",
        "                            token_type_ids.to(device))\n",
        "            \n",
        "            start_preds, end_preds, class_preds = (p.detach().cpu() for p in y_preds)\n",
        "            start_logits, start_index = torch.max(start_preds, dim=1)\n",
        "            end_logits, end_index = torch.max(end_preds, dim=1)\n",
        "\n",
        "            # span logits minus the cls logits seems to be close to the best\n",
        "            cls_logits = start_preds[:, 0] + end_preds[:, 0]  # '[CLS]' logits\n",
        "            logits = start_logits + end_logits - cls_logits  # (batch_size,)\n",
        "            indices = torch.stack((start_index, end_index)).transpose(0, 1)  # (batch_size, 2)\n",
        "            result.update(examples, logits.numpy(), indices.numpy(), class_preds.numpy())\n",
        "\n",
        "    return result.score()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sOIOKlE0RZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Result(object):\n",
        "    \"\"\"Stores results of all test data.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.examples = {}\n",
        "        self.results = {}\n",
        "        self.best_scores = defaultdict(float)\n",
        "        self.class_labels = ['LONG', 'UNKNOWN']\n",
        "        \n",
        "    @staticmethod\n",
        "    def is_valid_index(example: Example, index: List[int]) -> bool:\n",
        "        \"\"\"Return whether valid index or not.\n",
        "        \"\"\"\n",
        "        start_index, end_index = index\n",
        "        if start_index > end_index:\n",
        "            return False\n",
        "        if start_index <= example.question_len + 2:\n",
        "            return False\n",
        "        return True\n",
        "        \n",
        "    def update(\n",
        "        self,\n",
        "        examples: List[Example],\n",
        "        logits: torch.Tensor,\n",
        "        indices: torch.Tensor,\n",
        "        class_preds: torch.Tensor\n",
        "    ):\n",
        "        \"\"\"Update batch objects.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        examples : list of Example\n",
        "        logits : np.ndarray with shape (batch_size,)\n",
        "            Scores of each examples..\n",
        "        indices : np.ndarray with shape (batch_size, 2)\n",
        "            `start_index` and `end_index` pairs of each examples.\n",
        "        class_preds : np.ndarray with shape (batch_size, num_classes)\n",
        "            Class predicition scores of each examples.\n",
        "        \"\"\"\n",
        "        for i, example in enumerate(examples):\n",
        "            if self.is_valid_index(example, indices[i]) and \\\n",
        "               self.best_scores[example.example_id] < logits[i]:\n",
        "                self.best_scores[example.example_id] = logits[i]\n",
        "                self.examples[example.example_id] = example\n",
        "                self.results[example.example_id] = [\n",
        "                    example.doc_start, indices[i], class_preds[i]]\n",
        "\n",
        "    def _generate_predictions(self) -> Generator[Dict, None, None]:\n",
        "        \"\"\"Generate predictions of each examples.\n",
        "        \"\"\"\n",
        "        for example_id in self.results.keys():\n",
        "            doc_start, index, class_pred = self.results[example_id]\n",
        "            example = self.examples[example_id]\n",
        "            tokenized_to_original_index = example.tokenized_to_original_index\n",
        "            try:\n",
        "              short_start_index = tokenized_to_original_index[doc_start + index[0]]\n",
        "              short_end_index = tokenized_to_original_index[doc_start + index[1]]\n",
        "            except:\n",
        "              print(len(tokenized_to_original_index))\n",
        "              print(doc_start + index[1])\n",
        "              short_end_index = tokenized_to_original_index[len(tokenized_to_original_index) - 1]\n",
        "            long_start_index = -1\n",
        "            long_end_index = -1\n",
        "            for candidate in example.candidates:\n",
        "                if candidate['start_token'] <= short_start_index and short_end_index <= candidate['end_token']:\n",
        "                    long_start_index = candidate['start_token']\n",
        "                    long_end_index = candidate['end_token']\n",
        "                    break\n",
        "            yield {\n",
        "                'example': example,\n",
        "                'long_answer': [long_start_index, long_end_index]\n",
        "            }\n",
        "\n",
        "    def end(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Get predictions in submission format.\n",
        "        \"\"\"\n",
        "        preds = {}\n",
        "        for pred in self._generate_predictions():\n",
        "            example = pred['example']\n",
        "            long_start_index, long_end_index = pred['long_answer']\n",
        "\n",
        "            long_answer = f'{long_start_index}:{long_end_index}' if long_start_index != -1 else np.nan\n",
        "            preds[f'{example.example_id}_long'] = long_answer\n",
        "        return preds\n",
        "\n",
        "    def score(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate score of all examples.\n",
        "        \"\"\"\n",
        "\n",
        "        def _safe_divide(x: int, y: int) -> float:\n",
        "            \"\"\"Compute x / y, but return 0 if y is zero.\n",
        "            \"\"\"\n",
        "            if y == 0:\n",
        "                return 0.\n",
        "            else:\n",
        "                return x / y\n",
        "\n",
        "        def _compute_f1(answer_stats: List[List[bool]]) -> float:\n",
        "            \"\"\"Computes F1, precision, recall for a list of answer scores.\n",
        "            \"\"\"\n",
        "            has_answer, has_pred, is_correct = list(zip(*answer_stats))\n",
        "            precision = _safe_divide(sum(is_correct), sum(has_pred))\n",
        "            recall = _safe_divide(sum(is_correct), sum(has_answer))\n",
        "            f1 = _safe_divide(2 * precision * recall, precision + recall)\n",
        "            return f1\n",
        "\n",
        "        long_scores = []\n",
        "        short_scores = []\n",
        "        for pred in self._generate_predictions():\n",
        "            example = pred['example']\n",
        "            long_pred = pred['long_answer']\n",
        "\n",
        "            # long score\n",
        "            long_label = example.annotations['long_answer']\n",
        "            has_answer = long_label['candidate_index'] != -1\n",
        "            has_pred = long_pred[0] != -1 and long_pred[1] != -1\n",
        "            is_correct = False\n",
        "            if long_label['start_token'] == long_pred[0] and \\\n",
        "               long_label['end_token'] == long_pred[1]:\n",
        "                is_correct = True\n",
        "            long_scores.append([has_answer, has_pred, is_correct])\n",
        "\n",
        "        long_score = _compute_f1(long_scores)\n",
        "        return {\n",
        "            'long_score': long_score,\n",
        "            'overall_score': long_score \n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0LEWCzu0Z0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_reader = JsonChunkReader(DATA_PATH, convert_func, chunksize=chunksize)\n",
        "valid_data = next(data_reader)\n",
        "valid_data = list(itertools.chain.from_iterable(valid_data))\n",
        "valid_dataset = Subset(valid_data, range(len(valid_data)))\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=eval_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npWoc5zH6gXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "3954eb7a15714c65a421e35ead967ff6",
            "faedb5f98a57418584e7973cacf768a8",
            "f675c1056d7c4e77a13f79f85348e8ac",
            "bfd2441c1826404aa4c52de8f3170ef5",
            "b6c52763c98d496ba0446428e4cebb57",
            "93031049d808418cbab166e56978b178",
            "374df9eef5124ef59edf38dfa3684c95",
            "d96969a4b13448c5ba10dfff1c1eceff"
          ]
        },
        "outputId": "7e8d1341-1694-4616-d44e-675faeb8588e"
      },
      "source": [
        "eval_start_time = time.time()\n",
        "valid_scores = eval_model(model, valid_loader, device=device)\n",
        "print(f'calculate validation score done in {(time.time() - eval_start_time) / 60:.1f} minutes.')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3954eb7a15714c65a421e35ead967ff6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=860), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3537\n",
            "3714\n",
            "calculate validation score done in 4.0 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03lqpyXS0lHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7541fbf7-378b-4527-d073-988aa6e63c2a"
      },
      "source": [
        "long_score = valid_scores['long_score']\n",
        "overall_score = valid_scores['overall_score']\n",
        "print('validation scores:')\n",
        "print(f'\\tlong score    : {long_score:.4f}')\n",
        "print(f'\\toverall score : {overall_score:.4f}')\n",
        "print(f'all process done in {(time.time() - start_time) / 3600:.1f} hours.')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation scores:\n",
            "\tlong score    : 1.2121\n",
            "\toverall score : 1.2121\n",
            "all process done in 0.5 hours.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGeIy9h_2Zbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}